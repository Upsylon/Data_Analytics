---
title: "Project_Data_Anlytics"
author: "Bron_Luca, Grandadam_Patrik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
---

 - [Introduction](#Intro)
 - [Exploratory analysis](#Exp)
 - [Modelling](#Mod)
    - [CART](#CART)
    - [Neural Network](#NN)
    - [Support Vector Machine](#SVM)
    - [Random Forest](#RF)
    - [K-Nearest Neighbors](#KNN)
    - [....]()
 - [Cross Validation Approach](#CV)
 - [...](#)
 - [...](#)

**LE CODE EN COMMENTAIRE, C'EST JUSTE POUR POUVOIR KNIT PLUS VITE!!** (avec cache = TRUE, ca re-run pas le code, mais le markdown doit quand meme compiler les graphes, du coup ca prend du temps à chq fois...)


```{r include = FALSE}
knitr::opts_chunk$set(echo=TRUE)
```


```{r, message=FALSE, echo=FALSE}
cred <- read.csv2("GermanCredit.csv")
cred <- data.frame(cred)
attach(cred)
```


```{r, message=FALSE, echo=FALSE}
library(dplyr)
library(magrittr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(stringr)
library(DataExplorer)
library(corrr)
library(rpart)
library(rpart.plot)
library(maptree)
library(caret)
library(e1071)
library(randomForest)
library(gmodels)
library(MASS)
library(nnet)
library(neuralnet)
library(NeuralNetTools)
library(kknn)
```


```{r, echo=FALSE}
my_theme <- function(base_size = 10, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
    theme(
      axis.text = element_text(size = 10),
      axis.text.x = element_text(vjust = 0.5, hjust = 0.5),
      axis.title = element_text(size = 12),
      plot.title = element_text(hjust = 0.5),
      panel.grid.major = element_line(color = "grey"),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "aliceblue"),
      strip.background = element_rect(fill = "lightgrey", color = "grey", size = 1),
      strip.text = element_text(face = "bold", size = 10, color = "black"),
      legend.position = "bottom",
      legend.justification = "top", 
      legend.box = "horizontal",
      legend.box.background = element_rect(colour = "grey50"),
      legend.background = element_blank(),
      panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
    )
}
```

```{r}
names(cred) <- tolower(names(cred)) # changing the name of the variable into lowercase
cred <- cred[,-1] # deleting the first useless column
```


```{r, results='hide'}
sum(is.na(cred)) # checking if we have missing data
```

# Introduction {#Intro}

Parler de:
- méthode utilisée
- grandes étapes
- classification task (goal) =/= prediction
- [à compléter...]

--> Exploration des données = 1st insight
--> Modelling: train/test set + selection du "meilleur" arbre(pruning)/svm(tuning)/NNET(nb of layers)/regression logicstic/K-NN(choisir K)/etc. + presentation de détail de tout ce bordel
--> Cross validation avec les meilleurs de chq models pour selectionner le grand gagnant...

--> Description des variables (comme ca c'est plus simple de s'en sortir?!)


## Exploratory analysis {#Exp}

### Explanatory variables

#### Overview 

Before beginning any kind of analysis, we have to understand the data we are working with. 

```{r}
data.frame(variable = names(cred),
           classe = sapply(cred, typeof),
           first_values = sapply(cred, 
                                 function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% 
  kable(caption="Overview of our data") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F) %>%
  column_spec(1, width = "10em", border_right = T) %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "18em") %>%
  scroll_box(width = "65%", height = "250px")
```


&nbsp;

As we can see, the data are coherent with the infos that "the client" provided us. Most of them are binary or categorical, while only few are numerical.  

More than seeing the first values of our variables and their types, we also need to understand how distributed they are and their link with each other. Thanks to a correlation plot, we can see the correlation between each pair of variable, but especially their correlation with our response variable in which we are interested in.  
We see for instances that variables like *chk_acct*, *duration*, *history*, *sav_accnt* or *rent* are highly correlated (positively or negatively) with our outcome variable and that they will be likeli to influe it in the models that we are going to plot. Others like *present_resident* or *retaining* should have low impact.  

```{r}
plot_correlation(cred, type="all", title = "Correlation Graph") # Attention! Il faut seulement ploter avec les valeurs continues. Les catégoriques se feront avec le khi-2! Test de wilkockson pour tester si les groupes sont différents (comme je le comprends, voir si une valeur cotinue a une influence sur l'outcome des groupes). Ensuite, fait des tests d'indépendance sur les discrètes pour voir si elles ont une influence sur les valeurs de réponse. 
```


In addition, we can appreciate the summary of the different variables. The frequency table of *history* is presented below as an example:  

```{r, echo=FALSE}
c(summary(cred$history)) %>% as.data.frame %>%
  kable(caption = "Summary of variable history", 
        align = "l", col.names = c("Value")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "float_left"
  )

table(cred$history) %>% kable(caption = "Frequency table of history",
                              col.names = c("Values", "Frequency")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )
```


&nbsp;  
&nbsp;  


However, presenting such a summary for all variables can be long and boring. It can be better to represent these number visually. A Boxplot is optimal to get all the important values for the numerical data, while a barplot will give us strong insights for categorical data. Let's appreciate the following graphs:  

```{r, fig.height=3, fig.width=3, warning=FALSE, cache=TRUE}

for (i in 1:(length(cred)-1)) {
  if (range(cred[, i] < 5)) {
    print(
      ggplot(cred, aes(x = cred[, i])) +
        geom_bar(stat = "count", position = "dodge") +
        ggtitle(str_c("Barplot of\n", paste(
          colnames(cred[i])
        ))) +
        xlab(colnames(cred[i])) +
        ylab("Total") +
        my_theme()
    )
  } else
  {
    print(
      ggplot(cred, aes(y = cred[, i])) + geom_boxplot() +
        ylab(colnames(cred[i])) +
        ggtitle(str_c("Boxplot of\n ", paste(
          colnames(cred[i])
        ))) +
        my_theme() +
        theme(
          axis.text.x=element_blank())
    )
  }
}

```

Thanks to these graphs, we can better understand our data at a glance and will be able to refer to them when needed.  

In addition, these graphs enable un too see that some data are not tidy. For instance, *education* should be a binary variable. However, we can see on the histogram of this variable that we have data where $-1$ were recorded. We have the same problem for the binary variable *guarantor* were a value $2$ is present.  
In addition, we can also have strong suspitions that the variable *age* has wrong recorded data as we can see an outlier with a value much bigger than 100.  
We will have to confirm our first assumptions and to modify these dirty data in an appropriate way.  

Let's first look at our variable *age*. We assume that, generally, a person will not live more than a hundred year, and will not contract a credit at such age. This is why the data with $Age > 100$ are most likely wrongly recorded. We will therefore have to replace them in our database.  
First, we have to find how much data are potentially dirty according to our assumptions and to localise them in order to replace them.  


```{r, echo=FALSE}
attach(cred)
```

```{r, echo=FALSE}
table(age >= 100) %>% kable(caption = "Number of instances with age > 100") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "float_left") %>%
  column_spec(1, width = "5em", border_right = T) %>%
  column_spec(2, width = "5em")
which(age > 100) %>% kable(caption="Position of instance with age > 100") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "left") %>%
  column_spec(1, width = "9.5em") 
```

&nbsp;  

According to our results, we have one data with $age > 100$ that has to be replaced. It is the instance `r which(age>100) ` and its value is `r age[which(age>100)]`.  

We can consider different options to replace this value. The first one could be to replace it by a value at random within the range (a value at random between `r min(age)`  and `r max(age[age!=max(age)])`, which is the second lowest value after $125$.  
However, according to the following histogram, the distribution of the age (without the erroneous data) is inequal with a concentration around small values (which is logical as young people generally have less money than elders and therefore are more subject to ask for credits).  

```{r, fig.height=4, fig.width=4, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(cred, aes(x = cred$age)) + 
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of age without the erroneous data")) +
  xlab("Age") +
  ylab("Frequency") +
  xlim(c(min(age),max(age[age!=max(cred$age)]))) +
  my_theme()
```

It could therefore be possible to replace it at random with different probabilities according to the size of each class.  
We prefer to opt for the median (equal to `r median(age[age!=max(age)])`) to replace our problematic value as it offers more convenience.  
Note that for calculating the median, our problematic value should not be used.  

```{r}
cred$age[which(age>75)] <- median(age[age!=max(age)])
```

An alternative could have been to use the mean, but, as we have no really big outlier, both values would have been close to eachother ($mean = `r mean(age)`$ while $median = `r median(age)`$). 

&nbsp;  

Next, we also have to deal with our two categorical data that have been wrong recorded:  
    - one in *education*  
    - one in *guarantor*

They also have to be cleaned. 

The following is again the barplot of *education*. 

```{r, fig.height=4, fig.width=4, echo=FALSE}
ggplot(cred, aes(x = cred$education)) +
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of education")) +
  xlab("Education") +
  ylab("Count") +
  my_theme()
```

Here, the likelihood that this wrong recorded data is equal to $0$ is clearly higher. Therefore, each of the previously presented methods (using the mean, using the median and even assigning it to a class at random) would with a high probability result in assigning this instance and assign it the value $Education = 0$.  
We can confirm these first assumption with a frequency table:  

```{r, echo=FALSE}
edu <- rbind(table(education), 
             paste(round(prop.table(table(education)) * 100, 1), "%"))
rownames(edu) <- c("sample size", "proportion")
edu %>% kable(caption = "Frequency table of education", align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F) %>%
  column_spec(1, border_right = T)
```

&nbsp;

It is indeed more appropriate to replace our value by 0 as the probability of belonging to this class is close to 20 times bigger.  

```{r}
cred$education[which(education == -1)] <- 0
```

&nbsp;  

Concerning the variable *guarantor*, we can look at the frequency table and plot the barplot as well:  

```{r, echo=FALSE}
gua <- rbind(table(guarantor), 
             paste(round(prop.table(table(guarantor)) * 100, 1), "%"))
rownames(gua) <- c("sample size", "proportion")
gua %>% kable(caption = "Frequency table of guarantor", align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "float_left") %>%
  column_spec(1, border_right = T) %>%
  column_spec(2, width="5em") %>%
  column_spec(3, width="5em") 
```


```{r, fig.height=3, fig.width=3, echo=FALSE}
ggplot(cred, aes(x = cred$guarantor)) +
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of guarantor")) +
  xlab("Guarantor") +
  ylab("Count") +
  scale_x_continuous(breaks = c(0, 1, 2)) +
  my_theme()
```

Again, for the same reasons, it is preferable to replace the wrong recorded data by 0.  

```{r}
cred$guarantor[which(guarantor == 2)] <- 0
```

In addition, the variable *present_resident* is also problematic as it doesn't have the same range as the other categorical values. Its range goes from `r min(range(present_resident))` to `r max(range(present_resident))` whereas it should go from 0 to 3 like the other ones. We can modifiy its values in order to have the same format everywhere.
 
```{r}
cred$present_resident <- subtract(cred$present_resident, 1)
```

These first steps have enabled us to better understand our explanatory variables and to clean the problematic ones.  
We now have to focus in detail to the response variable on which the predictions should be made.   
 
### Response variable

As our final goal is to predict if a customer should be classified as a risky one or not, we have to have a particular look at our response variable that establishes if an applicant presents a good or a bad risk.  
Let's first have a look at its distribution:  

```{r, echo=FALSE}
cred$response <- as.factor(ifelse(cred$response == 1, "good", "bad")) 
ggplot(cred, aes(x = cred$response)) + 
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of the response variable")) +
  xlab("Response variable") +
  ylab("Total") +
  my_theme()
```

```{r, echo=FALSE}
res <- rbind(table(response), 
             paste(round(prop.table(table(response)) * 100, 1), "%"))
rownames(res) <- c("sample size", "proportion")
res %>% kable(caption = "Frequency table of our response variable", 
              align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F) %>%
  column_spec(1, border_right = T) %>%
  column_spec(2, width="5em") %>%
  column_spec(3, width="5em") 
```

As we can see, if a random customer steps in the bank, the *a priori* probability that he will present a good ranking will be of 70%.  
Without any calculations, the bank has more chances to make a good decision when octroying a credit.  
However, the consequences can be really dramatic if 30% of the credits that the bank gives are not totally reimbursed. That's why we have to develop a model to improve this initial accuracy that is obtained using a naive method of always octroying a credit.   
Optimally, this model should also minimise the number of credits that are predicted as "good" and that are actually "bad" as the consequences for the bank (reimbursement of the credit by the customer) can be much more dramatic in this situation than in a situation where a "bad" credit is predicted as "good".  

We will get to this later. First, after having presented each variable, it could be interesting to see if we can already have some assumptions concerning the relations between the explanatory variables and the response variable.

### Interactions between the explanatory variables and the response variable


Text... 
 
```{r, fig.width=3, fig.height=3, cache=TRUE, echo=FALSE}

for(i in 1:(length(cred) - 1)){
  print(
    ggplot(cred, aes(
      x = response, y = cred[, i] ,  group = response
    )) +
      geom_boxplot() +
      xlab("Response variable") +
      ylab(colnames(cred[i])) +
      ggtitle(str_c(
        "Interaction between\n ",
        paste(colnames(cred[i]), "and the\n response variable")
      )) +
      my_theme()
  )
}
```


--> Expliquer qu'on peut deja avoir un premier apriori sur les variables qui vont avoir un impact

** PAS TOP LES BOXPLOTS POUR LES VARIABLES BINAIRES ??!!** 


# Modelling {#Mod}

```{r}
for (i in 1:(length(cred)-1)) {
  if (range(cred[, i])[2] - range(cred[, i])[1] < 5) {
    cred[, i] <- cred[, i] %>% as.character %>% as.factor
  }
}
```

We can again look at our data to check if the categorical variables were indeed changed as factors.

```{r}
str(cred)
```


Before beginning to work on our different models, we can create a test set and a training set in order to build the models. This procedure is used is order to avoid overfitting and to predict instances which have been used while building the model.  
We opt for a size of 70% of the original dataset for the training set and a size of 30% of the original dataset for the testing set.  
In order to evalate the performance of the different models, we will have to use the exact **same training and test sets** for each model to be sure that the performance differences will result from the model we use and not from the randomess of splitting differently both sets.   
A Cross-Validation will be performed at the end in order to compare the different models and to choose which one should be used to make good predictions.  

```{r}
# Creation of testing and training sets
set.seed(123) 
index.train <- createDataPartition(y = cred$response, p = 0.7, list = FALSE)
cred.train <- cred[index.train,]
cred.test <- cred[-index.train,]
```


In addition, to evaluate our models, we will use the accuracy as main measure of performance. However, computing the accuracy for each model can be quite long... We prefer to build a function to be able to retrieve at any time based on a confusion matrix:  


```{r}
# Creating of a function to retrieve the accuracy from a confusion matrix
accuracy <- function(c){
  print(sum(diag(c)) / sum(c))
}
```

We can now begin to work on the different models that we will use in order to make our predictions.  

## CART {#CART}

First, we begin our analysis by using a decision (classification) tree.   

The goal of a decision tree is to predict the final class of our response variable ("good" or "bad") by using a succession of binary rules to apply to our data.   
Each node is created thanks to an algorithm that aims to minimize an impurity criterion. The feature and its underlying value  that maximises the impurity reduction (that "best splits"the dataset in two) will be selected. This procedure is repeated until a stopping rule is reached.  
At the end, we will have multiple branches that will all lead to the final forecast that we will make for a given instances.  
Better than words, let's compute the model and have a look at it.  

### Building the model

```{r}
cart.model <- rpart(response ~ .,  data = cred.train, method = "class")
```

```{r}
rpart.plot(cart.model, main = "Original decision tree")
```

--> At this time, we have obtained a decision tree that can be used to do the predictions. At it node, one should look at the values of the data and take the appropriate direction till arriving at the last row, when the prediction can be made.  
However, this tree is also really complex: there are a lot of split and of branches.  
We have to tune this initial model and make it more simple. We will simplify this model without loosing  predictive capabilities by pruning it, keeping only the most important splits linked to the most important variables.  

### Tuning the model: Pruning

As already said, our complex tree has to be pruned in order to reduce its complexity. To do so, we will use the *1 - SE rule*.   
The idea of this rule is really general. As one would establish a t-test in statistics to see if two measures are *statistically different*, the *1 - SE rule* tries to establish if two models produce statistically different results (if we can affirm that one outperforms the other).  
We therefore consider the *xerror* (the criterion in which we are interested in and that we want to minimize) and its standard deviation *xstd*. A models that falls within 1 standard deviation of the most complex model can be considered as equivalent in term of performance. Therefore, using this rule, we will be able to prune the tree to get a much simplier model, without loosing in quality as the performance capability of our new model will be *statistically the same* as the first one.  
To select the size of our pruned tree, we will look at the xerror of our original tree (the most complex one) and add one standard error *xstd*. We will then select the simplier tree with an xerror that lies within the calculated value.

Let's have a look at these values from our original tree and decide where to prune it.

```{r, echo=FALSE, fig.height=5, fig.width=5}
cart.model$cptable %>% 
  kable(caption = "CP table of the CART model") %>% 
  kable_styling(
  bootstrap_options = c("striped", "hover", "condensed"),
  full_width = F, 
  position = "float_left"
)
head(cart.model$variable.importance, n = 6) %>%
  kable(caption = "Variable importance table first 6 instances") %>% 
  kable_styling(
  bootstrap_options = c("striped", "hover", "condensed"),
  full_width = F
) %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "10em")

plotcp(cart.model)
```


```{r, echo = FALSE}
# xstd associated with lowest xerror
min.xstd <- cart.model$cptable[which.min(
  cart.model$cptable[,ncol(cart.model$cptable) -1]),
  ncol(cart.model$cptable)]

# lowest xerror
min.xerror <- cart.model$cptable[which.min(
  cart.model$cptable[,ncol(cart.model$cptable) -1]),
  ncol(cart.model$cptable) - 1]

sum.xe.xstd <- min.xstd + min.xerror # sum of both

# CP value to which to prune the tree ; 
# min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd)) 
# representing the row to be pruned at : 
cp.pruned <- 
  cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] 
                                < sum.xe.xstd))), 1]
```
 
Accoring to our first table, the minimal *xerror* is equal to `r cart.model$cptable[which.min(cart.model$cptable[,ncol(cart.model$cptable) -1]), ncol(cart.model$cptable) - 1]` , the minimal *xstd* to `r cart.model$cptable[which.min(cart.model$cptable[,ncol(cart.model$cptable) -1]), ncol(cart.model$cptable)]` and the sum of both is therefore equal to `r sum.xe.xstd`.  

The smallest tree with an xerror below this value is equal to `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))), ncol(cart.model$cptable) - 1]` and we will therefore prune the tree at CP = `r cp.pruned`. According to our previous table, this represents `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))), 2]  ` splits to be kept, equivalent to a tree of size `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))), 2] + 1  `.  
It is also possible to observe this value on the graph where the dash line indicates the *xerror* plus its *xstd* from the most complex tree. We therefore select the less complex tree under this line, which is the tree of size `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))),2] + 1  ` (or, again, with `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))),2]  ` splits).  

We thus prune the tree at this value (the *R* function requires to indicate CP.)

```{r}
cart.pruned <- prune(cart.model, cp = cp.pruned)
```

```{r, echo=FALSE}
rpart.plot(cart.pruned, main = "Pruned decision tree")
```

We can visualize this new pruned tree that is much smaller and therefore less complex than our original one and will use it to do our predictions, to build the confusion matrix and to calculate the underlying accuracy.  

### Predicting the values of the testing set

```{r, comment=NA}
pred.cart <- predict(cart.pruned, newdata = cred.test, type = "class")

# confusion matrix:
cart.tab <- table(Predictions = pred.cart, Observations = cred.test$response) 
cart.tab %>% kable(caption = "Confusion matrix for the CART model",
                   col.names = c("Predict bad", "Predict good")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l") %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")
```

Based on this table, we can calculate the accuracy using our previously build function, and that calculates the element weel classified divided by the total number of elements.  

```{r}
# accuracy using our previously build function
accuracy(cart.tab) 
```

It is possible to have more informaion, with for instance the sum of each rows with the following cross table.

```{r comment=NA, echo=FALSE, fig.height=3, fig.width=3}
CrossTable(x = cred.test$response, y = pred.cart, prop.chisq = FALSE)
```

We can see that `r cart.tab[1,1] + cart.tab[2,1]` are predicated as bad and `r cart.tab[1,2] + cart.tab[2,2]` are predicted as good. 


--> Parler de accuracy en elle meme (0.72 pas top par rapport aux 0.7 de base si on prédit que "good")
--> Parler des bonnes predictions de 0 (c'est elles qui nous intéressent et prédire que 1 conduirait la banque à faire faillite)
--> Parler **FALSE POSTIVIE** / FALSE NEGATIVE / TRUE POSITIVE / TRUE NEGATIVE (OU A LA FIN)


## Neural Network {#NN}

### Building the model and optimizing it: selecting the number of neurones in the hidden layer


**A MODIFIER // SUPPRIMER**
La "littérature" (https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) suggère de garder 1 seul layer un nb de neuronnes entre 1 et le nbr de dim: on va les tester tous et garder le meilleur...
"In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers."

"Over-parametrization quickly lead to instability in the estimate and can lead to overfitting. One possibility is to impose a penalty on the largest weights during the optimization. This is called regularization and, more specifically in the context of neural network, weight **decay.**"

--> On essaye de trouver le nombre "optimal" de neurones. Vu que neural network utilise des valeurs aléatoires au début pour lancer l'algo etc. Ca varie. On calcule donc 5 fois l'accuracy liée à chaque nombre de neurones et voir si y'a un nb de neurones optimal.



```{r, results = "hide"}
train_control <- trainControl(method = "cv", number = 5)

nnet_fit <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = expand.grid(size = 14:25, decay = c(1.0, 1.5, 2)),
                 MaxNWts = 3000,
                 trace = FALSE,
                 method = "nnet")

plot(nnet_fit)
nnet_fit$results
nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$size # the size maximizing the accuracy
nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$decay # the decay maximizing the accuracy
```

### Model selection and predicting the values of the testing set

We present you the characteristics of the model retained, with `r nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$size` neurones in the hidden layer and a decay of `r nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$decay`.
As usual, we trained the model and predict the values of the test set to be able to build the confusion matrix and to calculate the accuracy. 

```{r, results = 'hide'}
nnet.model.retained <- 
  nnet(cred.train$response ~ .,
       data = cred.train, 
       maxit = 200,
       MaxNWts = 3000,
       trace = FALSE,
       size = nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$size, 
       decay = nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$decay)

# Predictions on the test set:
pred.nnet.retained <- predict(nnet.model.retained, cred.test, type="class") 

# Confusion matrix
tab.nnet.retained <- table(Reality = cred.test$response, 
                           Predicted = unlist(pred.nnet.retained)) 
# Accuracy
acc.nnet.retained <- sum(ifelse(
  cred.test$response == unlist(pred.nnet.retained), 1, 0), na.rm = TRUE) /
  length(cred.test$response) 
```

In addition, we can plot our neural network:

```{r, comment = NA, fig.width=10, fig.height=10, echo=FALSE}
# plot
plotnet(nnet.model.retained, 
        alpha_val = 0.1,
        circle_col = "steelblue1", 
        pos_col = "blue", 
        neg_col = "red", 
        bord_col = "black", 
        pad_x = 0.8)
```

This graph shows that our model can be quite complex, there a plenty of arrows. It is not necessary to understand precisely all of them and they can be seen as a "black box", meaning that the interpretability of such a model is modest. But, afterall, what we need is to make good predictions!


```{r, echo=FALSE}
# confusion matrix
tab.nnet.retained %>% kable(
  caption = "Confusion matrix of the fitted NNET model",
  col.names = c("Predict good", "Predict bad")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")
```

We obtain a final accuracy of `r round(acc.nnet.retained,3)` on our testing set. 
Note that this accuracy is different from the one using the cross validation before as here, we use only one test set that may be disproportioned. We will, at the end, use again a cross validation for all models on the same train and test sets in order to compare the different models.  


## Support Vector Machine {#SVM}

Another important model that we will now consider and compare to the other ones will be the Support Vector Machines.  
This models aims at... **DESCRIPTION**

### Building the model

Again, we decide to use the *caret* package to be able to compare models with different costs. We will then select the best performing model and analyse it separately.  

```{r}
svm_parameters <- data.frame(C = seq(0.5, 5, 0.5)) 
# note that we first tested a wider scope of values (from 0 to 1000)
# and now present only the interval that leads to the best results

svm_fit <- train(form = response ~ .,
                   data = cred,
                   method = "svmRadialCost",
                   tuneGrid = svm_parameters,
                   preProcess = "range",
                   trace = FALSE,
                   trControl = train_control)
```

### Optimizing the model: selecting good cost parameter

Now that our different SVM models with different cost parameters have been built, we have to select the cost parameter that leads to the highest accuracy.

```{r}
plot(svm_fit)

svm_fit$results
svm_fit$results[which.max(svm_fit$results$Accuracy),]$C # the cost maximizing the accuracy
```


According to our results and by having tested different costs, we conclude that we should opt for a cost of `r svm_fit$results[which.max(svm_fit$results$Accuracy),]$C` which leads to the highest accuracy.  



We can then "extract" this model and analyze it on his own. We have to note that, as the Neural Network, SVM doesn't allow for much interpretations. What matters here is the quality of the predictions more than the individual interpretability of the different variables.  
In addition, to have a 2D graph, one should fix (slice) all other variables. Here, having 30 explanative variables, it does not make sense to fix 28 out of them and plotting is therefore not appropriate.  

However, we can look at the predictions of the model.

### Predicting the values of the testing set

```{r}
svm.model.retained <- 
  svm(cred.train$response ~ .,
       data = cred.train, 
       cost = svm_fit$results[which.max(svm_fit$results$Accuracy),]$C)

# Predictions on the test set:
pred.svm.retained <- predict(nnet.model.retained, cred.test, type="class") 

# Confusion matrix
tab.svm.retained <- table(Reality = cred.test$response, 
                           Predicted = unlist(pred.svm.retained)) 
# Accuracy
acc.svm.retained <- accuracy(tab.nnet.retained)
```

We can build the confusion matrix and see how the models performs.

```{r}
tab.svm.retained %>% kable(
  caption = "Confusion matrix of the fitted SVM model",
  col.names = c("Predict good", "Predict bad")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")
```

With this model we obtain a final accuracy of `r acc.svm.retained` on the original testing set that we built at the beginning (without CV).  
Again, we will compare this model with the others in a final step using a cross validation procedure performed on the same training sets and testing sets at the end of our analysis. 

## Random Forest {#RF}

Another model that we will consider is the Random Forest. Considering its construction, the Random Forest is a really special model.  
Indeed, the model is built **DESCRIPTION**....

Using our caret package again as our most powerful tool enabling to build an easy and readable code, we will test different *mtry* which correspond to number of variables that we consider at each split of the tree. Therefore, at each split, we consider only some variables (while in a regular CART model, we consider all of them), what will lead the different trees to be uncorrelated and increase their prediction power.  
** COMPLETER UN PEU...**

Finally, we will select the *mtry* that leads to the highest accuracy.  


```{r}
rf_parameters <- data.frame(mtry = c(7:14))
# note that we first tested a wider scope of values (from 0 to 1000)
# and now present only the interval that leads to the best results

rf_fit <- train(form = response ~ .,
                 data = cred,
                 method = "rf",
                 ntree = 500, 
                 tuneGrid = rf_parameters,
                 preProcess = "range",
                 trace = FALSE,
                 trControl = train_control)
```


We see that the error rate remains really similar past a certain value (around 100), meaning that increasing the number of tree does not allow to increase the predictive capabilities anymore. The second derivative is negative and the decrease of the error rate becomes smaller and smaller the more trees we add. We therefore decide to stick to this initial value of 500 trees. 


Now that we have built our model, we can select the optimal mtry to continue our analysis.  

```{r}
plot(rf_fit) 

rf_fit$results
rf_fit$results[which.max(rf_fit$results$Accuracy),]$mtry # the cost maximizing the accuracy
```

According to these results, we see that we should consider  `r rf_fit$results[which.max(rf_fit$results$Accuracy),]$mtry` variables at each split, which leads to the highest accuracy.

We can now analyse more in depth this model.

```{r}
rf.model.retained <- 
  randomForest(formula = response ~ .,
               data = cred.train, 
               ntree = 500, 
               tuneGrid = data.frame(
                 mtry = rf_fit$results[which.max(svm_fit$results$Accuracy),]$mtry),
               importance = TRUE)
```

Note that we decide not to make vary *ntree*, the number of trees used in the Random Forest and this number has been set to 500. This is the default number and increasing it more would have a quasi null impact. We can verify this assumption thanks to the following graph.

```{r}
plot(rf.model.retained)
```

Now that we have built our final model, contrarily to the regular CART, we can not directly see which variables are the most important looking at the branches of each tree.  
However, not like for the Neural Network or SVM, it is still possible to see the individual contribution of each variables thanks to the variable importance plot.

**INTERPRETATION DE COMMENT CA MARCHE** --> on retire un variable ca diminue --> variables importantes en haut

```{r}
varImpPlot(rf.model.retained) 
```


Finally, we can predict the values of our testing set, build the confusion matrix and predict the accuracy. 

```{r, echo = FALSE}
# Predictions on the test set:
pred.rf.retained <- predict(rf.model.retained, cred.test, type = "class") 

# Confusion matrix
tab.rf.retained <- table(Reality = cred.test$response, 
                           Predicted = unlist(pred.rf.retained)) 
# Accuracy
acc.rf.retained <- accuracy(tab.rf.retained)
```

```{r}
tab.rf.retained %>% kable(
  caption = "Confusion matrix of the fitted SVM model",
  col.names = c("Predict good", "Predict bad")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")
```

Using the Random Forest,  we obtain a final accuracy of `r acc.rf.retained` on the original testing set. 




















## K - Nearest Neighbors {#KNN}

### Building the model and Optimizing the model: selecting the number of neighbors and the distance measure

The following code will not be run each time we execute our evaluation of the different models in order to gain in computing efficiency (we have "*eval = FALSE*").  
However, we ran it one to be able to evaluate which distance measure to select.  

```{r, eval = FALSE}
knn_parameters <- expand.grid(kmax = 2:30, distance = 1:5, kernel= "optimal")

knn_fit <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = knn_parameters,
                 method = "kknn",
                 preProcess = c("center", "scale"))
plot(kknn_fit)
kknn_fit$results

kknn_fit$results[which.max(knn_fit$results$Accuracy),]$k
kknn_fit$results[which.max(knn_fit$results$Accuracy),]$distance
```

After having created multiple K-Nearest Neighbors models with the knn function, we realize that the distance of 2 outperforms other distances. We can therefore use the function knn to illustrate how our final model is built and how we select k (note that *knn* function is more efficient in term of computationnal power, only reason why we use it to choose k rather than the *kknn* function just above.) 

We can thus use the *knn* function to see how the accuracy varies with different k between 2 an 30 and select the number of neighbors that lead to the highest accuracy. 

```{r}
knn_parameters <- expand.grid(k = 2:30)

knn_fit <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = knn_parameters,
                 method = "knn",
                 preProcess = c("center", "scale"))
plot(knn_fit)
knn_fit$results

knn_fit$results[which.max(knn_fit$results$Accuracy),]$k
```

The selected knn model uses a distance of 2 and is computed with the `r knn_fit$results[which.max(knn_fit$results$Accuracy),]$k` nearest neighbors. The obtained accuracy using a 5 fold Cross-Validation is equal to `r max(knn_fit$results$Accuracy)`. 

### Predicting the values of the testing set

```{r, comment = NA, echo = FALSE}
pred.knn <- predict(knn_fit, newdata = cred.test)

# confusion matrix
knn.tab <- table(Predictions = pred.knn, Observations = cred.test$response) 
cart.tab %>% kable(caption = "Confusion matrix for the K-NN model",
                   col.names = c("Predict bad", "Predict good")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l") %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")
```

Given our confusion matrix, on our initial testing set, the obtained accuracy is equal to `r accuracy(knn.tab)`. 

































# Cross Validation approach {#CV}

At this point, let's make a summary of the situation.  
Until now, we have built several different models and we have tuned them (selected their parameters), so the different models that we have are *the best SVM*, *the best Neural Network* etc.  
However, we still have to confront each model to be able to select the *overall winner*.  
In order to be able to compare them, we will not evaluate them only on a testing as evaluating the performance of the model once will not enable us to make any conclusions. For instance, if the concerned testing set is disproportionned, we could attribute the good score of a model to the predictive capabilities of the model while it comes in fact from the test set.  
To be sure that the score of the model are not impacted (too much) by the characteristics of the testing set, we will perform a 10-fold Cross Validation.  
With this technique, we will use 10 different training set and 10 different testing sets. Our original dataset will be split in 10, using at each iteration 1/10 of the original dataset as a testing set and the rest as a training set. Using this method, each instance will be once (and exactly once) in the testing set.  
We will then be able to compute the accuracy on the different testing set for each iteration of the cross validation and then evaluate the average accuracy which will represent the numbers that we can expect the model to classify correctly. 

We will firstly make the whole procedure "manually" as an example and then use the famous *caret* package again to make the code as simple as possible. 

Thus, we begin with a creation of 10 different sets in order to do the Cross-Validation.

The number of instances being `r nrow(cred)`, we will make 10 sets of size `r nrow(cred)/10`. They are stored in a list named test.list. At each step, the remaining part of the data base is stored in the list train.test.

```{r}
test.list <- list() # creates an empty list that will be the test sets
train.list <- list() # creates an empty list that will be the train sets
counter <- 0

# Creates the 10 sets of size 300
for (i in 1:10){
  index <- counter + c(1:100) # the row numbers that will be in the test set
  test.list[[i]] <- cred[index, ] # the test set number i
  train.list[[i]] <- cred[-index, ] # the train set number i
  counter <- counter + 100
}
```

For example, *test.list[[1]]* is a data set of 300 rows taken at random from our data.

```{r} 
test.list[[1]] %>% kable(caption="Our first test set named test.list[[1]]") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = F) %>%
  scroll_box(width = "100%", height = "200px")


train.list[[1]] %>% kable(caption="Train set associated with test.list[[1]]") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = F) %>%
  scroll_box(width = "100%", height = "200px")
```

As already said, we first make the whole procedure on one model before using the caret package.  
We choose to present in details the CART model tuned and pruned.  
We have already built our 10 different training set and we will use them to make the predictions and calculate the accuracy of our 10 testing set. 

### CART Model

```{r, results = 'hide'}
# creating an empty vector to store the accuracy of each iteration
acc.cart.cv <- numeric(10) 

for (i in 1:10){
  cart.cv <- rpart(response~., data = train.list[[i]]) # original tree
  
  # Tree to be prune at: 
  cp.pruned <- cart.cv$cptable[(
    min(which(cart.cv$cptable[, ncol(cart.cv$cptable) - 1] < 
                cart.cv$cptable[nrow(cart.cv$cptable), 
                                   ncol(cart.cv$cptable)] + 
                cart.cv$cptable[nrow(cart.cv$cptable), 
                                   ncol(cart.cv$cptable) - 1]))), 1] 
  
  # the pruned tree for predictions
  cart.pruned.cv <- prune(cart.cv, cp = cp.pruned) 
  
  # making the predictions
  cart.pred.cv <- predict(cart.pruned.cv, newdata = test.list[[i]], type = "class") 
  
  # the confusion matrix
  tab.cart.cv <- table(test.list[[i]]$response, cart.pred.cv) 
  
  # the final accuracy
  acc.cart.cv[i] <- accuracy(tab.cart.cv) 
}
```

We can now extract what we are interested in: informations about the accuracy.

```{r}
acc.cart.cv # the accuracies obtained on each testing set
mean(acc.cart.cv) # the average accuracy
sd(acc.cart.cv) # the standard deviation of the accuracies
```
 
Here, we have an average accuracy for the *CART* model which is equal to `r mean(acc.cart.cv)` and that has a standard deviation of `r sd(acc.cart.cv)`. We see that, using a cross validation, the accuracy obtained differ from the one using only the testing/training set approach. However, it is of course still really close to the previous one and within a range of one standard deviation, which was also completely predictable.  
Now that we have seen in detail how the cross validation works thanks to this hand-made code, we can prefer to use the caret package that will do the work for us.  
Defining *train_control*, we will be able to use the same split between testing and training set for all the models. We will thus be able to conclude that differences in the accuracy will indeed be the consequence of the predictive capabilities of the model, and not from a "bad" repartition of the instances in the two sets.  
Let's run the cross validation on each model that we previously tuned. 

```{r, results = 'hide'}
train_control <- trainControl(method = "cv", number = 10)

rpart_cv <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 method = "rpart1SE")

nnet_cv <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = 
                   data.frame(
                     size = nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$size,
                     decay = nnet_fit$results[which.max(nnet_fit$results$Accuracy),]$decay),
                 MaxNWts = 3000,
                 trace = FALSE,
                 method = "nnet")

svm_cv <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = data.frame(
                   C = svm_fit$results[which.max(svm_fit$results$Accuracy),]$C),
                 method = "svmRadialCost")

knn_cv <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = data.frame(
                   k = knn_fit$results[which.max(knn_fit$results$Accuracy),]$k),
                 method = "knn",
                 preProcess = c("center", "scale"))

rf_cv <- train(form = response ~ .,
                 data = cred,
                 method = "rf",
                 ntree = 500, 
                 tuneGrid = data.frame(
                   mtry=rf_fit$results[which.max(rf_fit$results$Accuracy),]$mtry),
                 preProcess = "range",
                 trace = FALSE,
                 trControl = train_control)

cart_cv <- train(form = response~ .,
                 data = cred,
                 trControl = train_control,
                 tuneGrid = data.frame(
                   k = knn_fit$results[which.max(knn_fit$results$Accuracy),]$k),
                 method = "knn",
                 preProcess = c("center", "scale"))
```

```{r}
rpart_cv$results$Accuracy
nnet_cv$results$Accuracy
svm_cv$results$Accuracy
knn_cv$results$Accuracy
rf_cv$results$Accuracy
```

```{r}
confusionMatrix(rpart_cv)$table
confusionMatrix(nnet_cv)$table
confusionMatrix(svm_cv)$table
confusionMatrix(knn_cv)$table
confusionMatrix(rf_cv)$table
```

