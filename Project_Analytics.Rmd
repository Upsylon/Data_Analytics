---
title: "Project_Data_Anlytics"
author: "Bron_Luca, Grandadam_Patrik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

 - [Introduction](#Intro)
 - [Exploratory analysis](#Exp)
 - [Modelling](#Mod)
    - [CART](#CART)
    - [Logistic Regression](#Logit)
    - [....]()
    - [....]()
    - [....]()
 - [Cross Validation Approach](#CV)
 - [...](#)
 - [...](#)

**LE CODE EN COMMENTAIRE, C'EST JUSTE POUR POUVOIR KNIT PLUS VITE!!** (avec cache = TRUE, ca re-run pas le code, mais le markdown doit quand meme compiler les graphes, du coup ca prend du temps à chq fois...)


```{r include = FALSE}
knitr::opts_chunk$set(echo=FALSE)
```


```{r, message=FALSE, echo=FALSE}
cred <- read.csv2("GermanCredit.csv")
cred <- data.frame(cred)
attach(cred)
```


```{r, message=FALSE, echo=FALSE}
library(dplyr)
library(magrittr)
library(knitr)
library(kableExtra)
library(ggplot2)
library(stringr)
library(DataExplorer)
library(corrr)
library(rpart)
library(rpart.plot)
library(maptree)
library(caret)
library(gmodels)
library(MASS)
library(nnet)
library(neuralnet)
library(NeuralNetTools)
library(kknn)
```


```{r, echo=FALSE}
my_theme <- function(base_size = 10, base_family = "sans"){
  theme_minimal(base_size = base_size, base_family = base_family) +
    theme(
      axis.text = element_text(size = 10),
      axis.text.x = element_text(vjust = 0.5, hjust = 0.5),
      axis.title = element_text(size = 12),
      plot.title = element_text(hjust = 0.5),
      panel.grid.major = element_line(color = "grey"),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = "aliceblue"),
      strip.background = element_rect(fill = "lightgrey", color = "grey", size = 1),
      strip.text = element_text(face = "bold", size = 10, color = "black"),
      legend.position = "bottom",
      legend.justification = "top", 
      legend.box = "horizontal",
      legend.box.background = element_rect(colour = "grey50"),
      legend.background = element_blank(),
      panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
    )
}
```

```{r}
names(cred) <- tolower(names(cred)) # changing the name of the variable into lowercase
cred <- cred[,-1]
cred$response <- as.factor(ifelse(cred$response==1, "good", "bad"))
```


```{r, results='hide'}
sum(is.na(cred)) # checking if we have missing data
```

# Introduction {#Intro}

Parler de:
- méthode utilisée
- grandes étapes
- classification task (goal) =/= prediction
- [à compléter...]

--> Exploration des données = 1st insight
--> Modelling: train/set set + selection du "meilleur" arbre(pruning)/svm(tuning)/NNET(nb of layers)/regression logicstic/K-NN(choisir K)/etc. + presentation de détail de tout ce bordel
--> Cross validation avec les meilleurs de chq models pour selectionner le grand gagnant...

--> Description des variables (comme ca c'est plus simple de s'en sortir?!)


## Exploratory analysis 

### Explanatory variables

Before beginning any kind of analysis, we have to understand the data we are working with. 

```{r}
data.frame(variable = names(cred),
           classe = sapply(cred, typeof),
           first_values = sapply(cred, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% 
  kable(caption="Overview of our data") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1, width = "10em", border_right = T) %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "18em") %>%
  scroll_box(width = "70%", height = "200px")
  
```


&nbsp;

As we can see, the data are coherent with the infos that "the client" provided us. Most of them are binary of categorical, while only few are numerical.  

More than seeing the first values of our variables and their types, we also need to understand how distributed they are and their link with each other. Thanks to a correlation plot, we can see the correlation between each pair of variable, but especially their correlation with our response variable in which we are interested in.  
We see for instances that variables like *chk_acct*, *duration*, *history*, *sav_accnt* or *rent* are highly correlated (positively or negatively) with our outcome variable and that they will be likeli to influe it in the models that we are going to plot. Others like *present_resident* or *retaining* should have low impact.  

```{r}
plot_correlation(cred, type="all", title = "correlation graph")
```




In addition, we can appreciate the summary of the different variables. The frequency table of *history* is presented below as an example:  

```{r}
c(summary(cred$history)) %>% as.data.frame %>%
  kable(caption = "Summary of variable history", align = "l", col.names = c("Value")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "float_left"
  )

table(cred$history) %>% kable(caption = "Frequency table of history",
                              col.names = c("Values", "Frequency")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )
```


&nbsp;  
&nbsp;  

However, presenting such a summary for all variables can be long and boring. It can be better to represent these number visually. A Boxplot is optimal to get all the important values for the numerical data, while a barplot will give us strong insights for categorical data. Let's appreciate the following graphs:  

```{r, fig.height=3, fig.width=3, warning=FALSE, cache=TRUE}

# for (i in 1:(length(cred)-1)) {
#   if (range(cred[, i] < 5)) {
#     print(
#       ggplot(cred, aes(x = cred[, i])) + geom_bar(stat = "count", position = "dodge") +
#         ggtitle(str_c("Barplot of\n", paste(
#           colnames(cred[i])
#         ))) +
#         xlab(colnames(cred[i])) +
#         ylab("Total") +
#         my_theme()
#     )
#   } else
#   {
#     print(
#       ggplot(cred, aes(y = cred[, i])) + geom_boxplot() + 
#         ylab(colnames(cred[i])) +
#       ggtitle(str_c("Boxplot of\n ", paste(
#         colnames(cred[i])
#       ))) +
#         my_theme() +
#         theme(
#         axis.text.x=element_blank())
#     )
#   }
# }

```

Thanks to these graphs, we can better understand our data at a glance and will be able to refer to them when needed.  

In addition, these graphs enable un too see that some data are not tidy. For instance, *education* should be a binary variable. However, we can see on the histogram of this variable that we have data where $-1$ were recorded. We have the same problem for the binary variable *guarantor* were a value $2$ is present.  
In addition, we can also have strong suspitions that the variable *age* has wrong recorded data as we can see an outlier with a value much bigger than 100.  
We will have to confirm our first assumptions and to modify these dirty data in an appropriate way.  

Let's first look at our variable *age*. We assume that, generally, a person will never live more than a hundred year, and will never contract a credit at such ages. This is why the data with $Age > 100$ are definitely wrong recorded. We will therefore have to replace them in our database.  
First, we have to find how much data are potentially dirty according to our assumptions and to localisate them in order to replace them.  

```{r}
attach(cred)
```


```{r}
table(age>=100) %>% kable(caption="Number of instances with age > 100") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "float_left") %>%
  column_spec(1, width = "5em", border_right = T) %>%
  column_spec(2, width = "5em")
which(age>100) %>% kable(caption="Position of instance with age > 100") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "left") %>%
  column_spec(1, width = "9.5em") 
```

&nbsp;  
&nbsp;  


According to our results, we have one data with $age > 100$ that has to be replaced. It is the instance `r which(age>100) ` and its value is `r age[which(age>100)]`.  

We can consider different options to replace this value. The first one could be to replace it by a value at random within the range (a value at random between `r min(age)`  and `r max(age[age!=max(age)])`, which is the second lowest value after $125$.  
However, according to the following histogram, the distribution of the age (without the erroneous data) is inequal with a concentration around small values.  

```{r, fig.height=4, fig.width=4, message=FALSE, warning=FALSE}
ggplot(cred, aes(x = cred$age)) + 
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of age without the erroneous data")) +
  xlab("Age") +
  ylab("Frequency") +
  xlim(c(min(age),max(age[age!=max(cred$age)]))) +
  my_theme()
```

It could therefore be possible to replace it at random with different probabilities according to the size of each class.  
We prefer to opt for the median (equal to `r median(age[age!=max(age)])`) to replace our problematic value as it offers more convenience.  
Note that for calculating the median, our problematic value should not be used.  

```{r}
cred$age[which(age>75)] <- median(age[age!=max(age)])
```

An alternative could have been to use the mean, but, as we have no really big outlier, both values would have been close to eachother ($mean = `r mean(age)`$ while $median = `r median(age)`$).  


Next, we also have to deal with our two categorical data that have been wrong recorded:  
    - one in *education*  
    - one in *guarantor*

They also have to be cleaned. 

The following is again the barplot of *education*. 

```{r, fig.height=4, fig.width=4}
ggplot(cred, aes(x = cred$education)) +
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of education")) +
  xlab("Education") +
  ylab("Count") +
  my_theme()
```

Here, the likelihood that this wrong recorded data is equal to $0$ is clearly higher. Therefore, each of the previously presented methods (using the mean, using the median and even assigning it to a class at random) would with a high probability result in assigning this instance and assign it the value $Education = 0$.  
We can confirm these first assumption with a frequency table:  

```{r}
edu <- rbind(table(education), paste(round(prop.table(table(education)) * 100, 1), "%"))
rownames(edu) <- c("sample size", "proportion")
edu %>% kable(caption = "Frequency table of education", align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  column_spec(1, border_right = T)
```


&nbsp;

It is indeed more appropriate to replace our value by 0 as the probability of belonging to this class is close to 20 times bigger.  

```{r}
cred$education[which(education==-1)] <- 0
```


Concerning the variable *guarantor*, we can look at the frequency table and plot the barplot as well:  

```{r}
gua <- rbind(table(guarantor), paste(round(prop.table(table(guarantor)) * 100, 1), "%"))
rownames(gua) <- c("sample size", "proportion")
gua %>% kable(caption = "Frequency table of guarantor", align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F, position = "float_left") %>%
  column_spec(1, border_right = T) %>%
  column_spec(2, width="5em") %>%
  column_spec(3, width="5em") 
```


```{r, fig.height=3, fig.width=3}
ggplot(cred, aes(x = cred$guarantor)) +
  geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of guarantor")) +
  xlab("Guarantor") +
  ylab("Count") +
  scale_x_continuous(breaks = c(0, 1, 2)) +
  my_theme()
```

Again, for the same reasons, it is preferable to replace the wrong recorded data by 0.  

```{r}
cred$guarantor[which(guarantor==2)] <- 0
```

In addition, the variable *present_resident* is also problematic as it doesn't have the same range as the other categorical values. Its range goes from `r min(range(present_resident))` to `r max(range(present_resident))` whereas it should go from 0 to 3 like the other ones. We can modifiy its values in order to have the same format everywhere.
 
```{r}
cred$present_resident <- cred$present_resident - 1
```
 
 
**Variable num_credits: y'a pas de 0, pas cohérent...**
 
These first steps have enabled us to better understand our explanatory variables and to clean the problematic ones.
 
### Response variable

As our final goal is to predict if a customer should be classified as a risky one or not, we have to have a particular look at our response variable that establishes if an applicant presents a good or a bad risk.  

```{r}
ggplot(cred, aes(x = cred$response)) + geom_bar(stat = "count", position = "dodge") +
  ggtitle(str_c("Barplot of the response variable")) +
  xlab("Response variable") +
  ylab("Total") +
  my_theme()
```

```{r}
res <- rbind(table(response), paste(round(prop.table(table(response)) * 100, 1), "%"))
rownames(res) <- c("sample size", "proportion")
res %>% kable(caption = "Frequency table of our response variable", align = 'c') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                full_width = F) %>%
  column_spec(1, border_right = T) %>%
  column_spec(2, width="5em") %>%
  column_spec(3, width="5em") 
```

As we can see, if a random customer steps in the bank, the *a priori* probability that he will present a good ranking will be of 70%.  
Without any calculations, the bank has more chances to make a good decision when octroying a credit.  
However, the consequences can be really dramatic if 30% of the credits that the bank gives are not totally reimbursed. That's why we have to develop a model that will have an accuracy much better than these initial 70% that are obtained using a naive method of always octroying a credit.  
Better, our model should 

We will get to this later. First, after having presented each variable, it could be interesting to see if we can already have some assumptions concerning the relations between the expanatory variables and the response variable.

### Interactions between the explanatory variables and the response variable


Text... 
 
```{r, fig.width=3, fig.height=3, cache=TRUE }

for(i in 1:(length(cred) - 1)){
  print(
    ggplot(cred, aes(
      x = response, y = cred[, i] ,  group = response
    )) +
      geom_boxplot() +
      xlab("Response variable") +
      ylab(colnames(cred[i])) +
      ggtitle(str_c(
        "Interaction between\n ",
        paste(colnames(cred[i]), "and the\n response variable")
      )) +
      my_theme()
  )
}

```

--> Expliquer qu'on peut deja avoir un premier apriori sur les variables qui vont avoir un impact










# Modelling {#Mod}

Creation de test et train set:

```{r}
set.seed(111)
index.train <- sample(1:nrow(cred), size=nrow(cred)*0.75, replace=FALSE)
cred.train <- cred[index.train,]
cred.test <- cred[-index.train,]
```





## CART {#CART}

** Petite description de CART, ...**

### Building the model


```{r}
cart.model <- rpart(response ~ .,  data = cred.train, method = "class")
```


```{r}
rpart.plot(cart.model, main="Original decision tree")
```

```{r}
cart.model$cptable %>% 
  kable(caption = "CP table of the CART model") %>% 
  kable_styling(
  bootstrap_options = c("striped", "hover", "condensed"),
  full_width = F, 
  position = "float_left"
)
head(cart.model$variable.importance, n = 6) %>%
  kable(caption = "Variable importance table first 6 instances") %>% 
  kable_styling(
  bootstrap_options = c("striped", "hover", "condensed"),
  full_width = F
) %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "10em")

plotcp(cart.model)
```

--> Bon model mais bcp trop complexe... On doit pruner. Idee: on veut un arbre plus simple sans perdre en qualité prédictive (on peut pas dire statistiquement que 1 est meilleur que l'autre etc.)
--> Parler des variables les plus importantes

### Optimizing the model: Pruning

1 - SE rule... 
--> Parler de l'idée générale: simplifier le modele sans perdre en qualité
--> Dire comment ca marche

```{r}
min.xstd <- cart.model$cptable[nrow(cart.model$cptable), ncol(cart.model$cptable)] # xstd associated with lowest xerror
min.xerror <- cart.model$cptable[nrow(cart.model$cptable), ncol(cart.model$cptable) - 1] # lowest xerror
sum.xe.xstd <- min.xstd + min.xerror # sum of both
cp.pruned <- cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))),1]+10^-10 # CP value to which to prune the tree ; min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd)) representing the row to be pruned at
```

Pruning the tree at CP = `r cp.pruned`. According to our previous table, this represents `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))),2]  ` splits to be kept, equivalent to a tree of size `r cart.model$cptable[(min(which(cart.model$cptable[,ncol(cart.model$cptable) -1] < sum.xe.xstd))),2] + 1  `.

```{r}
cart.pruned <- prune(cart.model, cp=cp.pruned)
cart.pruned$cptable %>% 
  kable(caption = "CP table of the CART model") %>% 
  kable_styling(
  bootstrap_options = c("striped", "hover", "condensed"),
  full_width = F, 
  position = "float_left"
)
rpart.plot(cart.pruned, main="Pruned decision tree")
```


### Predicting the values of the testing set

```{r}
pred.cart <- predict(cart.pruned, newdata = cred.test, type = "class")
cart.tab <- table(Predictions = pred.cart, Observations = cred.test$response) # confusion matrix.
cart.tab %>% kable(caption = "Confusion matrix for the CART model",
                   col.names = c("Predict bad", "Predict good")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l") %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")

# sum(diag(cart.tab))/sum(cart.tab) # accuracy

```

Accuracy of the pruned CART model on the test set: `r sum(diag(cart.tab))/sum(cart.tab)`.

More details: 


```{r comment=NA}
CrossTable(x = cred.test$response, y = pred.cart, prop.chisq = FALSE)
```

--> 18 sur 56 sont mal prédit parmis les 0 (grosse perte --> non remboursement ou remboursement partiel si on leur prete) et 47 sur 194 sont mal prédits par parmis le 1 (petite perte --> on perd juste les intérets si on décide de pas prêter). 



--> Parler de accuracy en elle meme (0.74 pas top par rapport aux 0.7 de base si on prédit que "good")
--> Parler des bonnes predictions de 0 (c'est elles qui nous intéressent et prédire que 1 conduirait la banque à faire faillite)
--> Parler **FALSE POSTIVIE** / FALSE NEGATIVE / TRUE POSITIVE / TRUE NEGATIVE








## Logistic regression {#Logit}

--> Parler de pourquoi Logistic regression (fait un bon taf quand outcome binaire)



### Building the model

```{r comment=NA}
logit.model <- glm(response ~., data=cred.train, family="binomial")
summary(logit.model)
```

--> Parler du fait qu'on a des similitudes avec CART (les variables importantes avant ont aussi des coeffs signi ici... ** pas toutes... ** --> nuancer...).
--> Parler du fait qu'on a aussi plein de variables inutiles et qu'on va faire une stepwise pour minimiser AIC 
--> Dire l'idée de AIC (arbritrage entre perf du model et complexité)
--> Expliquer comment ca marche

### Optimizing the model: Stepwise selection according to AIC criteria


```{r, results='hide', echo=TRUE}
logit.step <- step(logit.model, direction="both")
summary(logit.step)
```


--> Parler du fait qu'on a quand meme encore énormément de variables mais que le AIC est minimisé
--> Parler du fait qu'on pourrait en virer manuellement (mais long et chiant)

### Predicting the values of the testing set


```{r}
pred.logit.step <- ifelse(
  predict(logit.step, newdata=cred.test, type="response") > 0.5, "good", "bad"
  ) # the predictions of the class
pred.logit.step.prob <- predict(logit.step, newdata=cred.test, type="response")

data.frame(pred.logit.step.prob,cred.test$response) %>% 
  ggplot(aes(x=cred.test$response, y = pred.logit.step.prob, group=cred.test$response)) + geom_boxplot() +
        my_theme() +
  ggtitle("Predicted probabilities against real values") +
  xlab("Values of the testing set") +
  ylab("Predicted probabilities of the logistic regression") 
```

```{r}
glm.tab <-
  table( Observations = cred.test$response, Predictions = pred.logit.step) # confusion matrix.
glm.tab %>% kable(caption = "Confusion matrix for the Logistic regression model",
                  col.names = c("Predict 0", "Predict 1")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")

# sum(diag(glm.tab))/sum(glm.tab) # accuracy
```

```{r comment=NA}
CrossTable(x = cred.test$response, y = pred.logit.step, prop.chisq = FALSE)
```

FALSE POSITIVE: 47 sur les 250 sont prédits 1 alors qu'ils sont 0... C'est la merde
Idée: on peut changer la proba limite pour dire que c'est "bon" pour avoir moins de FALSE POSITIVE...
Ca va baisser l'accuracy, mais ca va potentiellement nous faire perdre moins d'argent (on va refuser plus souvent les crédits à risques, et aussi les crédits sans risques...)


```{r}
pred.logit_0.7 <- ifelse(
  predict(logit.step, newdata=cred[-index.train,], type="response") > 0.75, "good", "bad"
  ) # the predictions of the class
```

```{r}
glm.tab <-
  table(Observations = cred.test$response, Predictions = pred.logit_0.7) # confusion matrix.
glm.tab %>% kable(caption = "Confusion matrix for the adapted Logistic regression model",
                  col.names = c("Predict bad", "Predict good")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) %>%
  column_spec(1, border_right = T, width = "5em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em")

sum(diag(glm.tab))/sum(glm.tab) # accuracy
```

--> En passant la limite à 0.75, on a gardé une bonne accuracy, et on fait moins d'erreurs d'évaluation parmis les "risky". Faut voir se que la banque peut se permettre comme marge et si les risky conduisent forcément à des pertes...


## Neural Network 

### Building the model and optimizing it: selecting the number of neurones in the hidden layer

La "littérature" (https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) suggère de garder 1 seul layer un nb de neuronnes entre 1 et le nbr de dim: on va les tester tous et garder le meilleur...
"In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers."

```{r, results='hide'}
# # creating our 30 models (with 1 to 30 neurones in the hidden layer)
# nnet.model = list()
# for (i in 1:30) {
#   nnet.model[[i]] = nnet(cred.train$response~., data = cred.train, size=i, maxit=200, decay = 1)
# }
# 
# # making the predictions on the testing set
# pred.nnet=list()
# for (i in 1:30){
#   pred.nnet[i] = predict(nnet.model[i], cred.test)
#   pred.nnet[[i]] <- ifelse(pred.nnet[[i]]  > 0.5 , "good",  "bad")
# }
# 
# # creating the confusion matrices
# tab.nnet=list()
# for (i in 1:30){
#   tab.nnet[[i]] <- table(Reality=cred.test$response, Predicted=unlist(pred.nnet[i]))
# }
# 
# # calculating the accuracy
# acc.nnet = list()
# for (i in 1:30) {
#   acc.nnet[i] <-
#     sum(ifelse(cred.test$response == unlist(pred.nnet[i]), 1, 0), na.rm = TRUE) /
#     length(cred.test$response)
# }
```

```{r, results='hide', cache=TRUE}
# # Creating a multitude of models with variations regarding the number of neurones in the hidden layer and regarding the decay. We try as many neurones as we have variables (30) and different 7 decay, meaning that this operation computes 210 models and can take quite a while (count 1 to 2 sec per model). But this is the price to pay to have a well tuned model!!
# nnet.model = list()
# k <- 0
# for (j in 2 ^ (-6:0)) {
#   ifelse(k<=8, k <- k+1, k <- 1)
#   for (i in 1:length(cred)) {
#     nnet.model[[10 * i + k]] <- nnet(cred.train$response ~ ., data = cred.train,  maxit = 200,
#                                     size = i,
#                                     decay = j
#     )
#   }
# }
# 
# nnet.model[sapply(nnet.model,is.null)] <- NULL
# 
# # making the predictions on the testing set
# pred.nnet=list()
# for (i in 1:length(nnet.model)){
#   pred.nnet[i] = predict(nnet.model[i], cred.test)
#   pred.nnet[[i]] <- ifelse(pred.nnet[[i]]  > 0.5 , "good",  "bad")
# }
# 
# # creating the confusion matrices
# tab.nnet=list()
# for (i in 1:length(nnet.model)){
#   tab.nnet[[i]] <- table(Reality=cred.test$response, Predicted=unlist(pred.nnet[i]))
# }
# 
# # calculating the accuracy
# acc.nnet = list()
# for (i in 1:length(nnet.model)) {
#   acc.nnet[i] <-
#     sum(ifelse(cred.test$response == unlist(pred.nnet[i]), 1, 0), na.rm = TRUE) /
#     length(cred.test$response)
# }
```

After having created different models with a different number of neurons in the hidden layers, we have to be able to access the created models and to select the best one based on it.  

### Model selection and predicting the values of the testing set

We present you the characteristics of the model with ** REMETTRE BIEN ICI...** r which.max(acc.nnet) neurons in the hidden layer and that proposes the highest accuracy on the testing set among all.

We can see that the plot of model can be quite complex, there a plenty of arrows... It is not necessary to understand precisely all of them and they can be seen as a "black box", meaning that the interpretability of such a model is modest. But, afterall, what we need is to make good predictions!


```{r, comment = NA, fig.width=10, fig.height=10}
# selected.nnet <- nnet.model[[which.max(acc.nnet)]]
# paste("The selected model has", selected.nnet$n[2]) %>% paste("neurones in the hidden layer and a decay of", selected.nnet$decay)
# 
# # plot
# plotnet(nnet.model[[which.max(acc.nnet)]], alpha_val=0.1,
#         circle_col="steelblue1", pos_col="blue", neg_col="red", bord_col="black")
# 
# # confusion matrix
# tab.nnet[[which.max(acc.nnet)]] %>% kable(caption = "Confusion matrix of the fitted NNET model",
#                                           col.names = c("Predict good", "Predict bad")) %>%
#   kable_styling(
#     bootstrap_options = c("striped", "hover", "condensed"),
#     full_width = F,
#     position = "l"
#   ) %>%
#   column_spec(1, border_right = T, width = "5em") %>%
#   column_spec(2, width = "6em") %>%
#   column_spec(3, width = "6em")
# 
# # accuracy
# paste("The obtained accuracy is equal to", acc.nnet[[which.max(acc.nnet)]]) # outstanding compared to our previous models!

```

We obtain a final accuracy of ** REMETTRE BIEN ICI...** r acc.nnet[[which.max(acc.nnet)]] on our testing set.










## Support Vector Machine (SVM)

### Building the model

### Optimizing the model: selecting good cost and gamma parameters

### Predicting the values of the testing set


## Model x (K-nn)

### Building the model

```{r}
scaled.cred <- scale(cred[,-length(cred)]) %>% data.frame(response=cred$response)
# scaling except last row (except response)

scaled.cred.train <- scale(cred.train[,-length(cred.train)]) %>% data.frame(response=cred.train$response)
scaled.cred.test <- scale(cred.test[,-length(cred.test)]) %>% data.frame(response=cred.test$response)
```

```{r}
# Creation of 20 KNN: 10 with i={1,..,10} and k = 2 and 10 with i={1,..,10} and k = 3 
knn.model = list()
k <- 0
for (j in 2:3) {
  for (i in 1:20) {
    knn.model[[20 * k + i]] <- kknn(response ~ ., k=i, distance=j,train=scaled.cred.train, test=scaled.cred.test)
  }
  k <- k+1
}

# making the predictions on the testing set
pred.knn=list()
for (i in 1:length(knn.model)){
  pred.knn[[i]] = table(Prediction=knn.model[[i]]$fitted.value, Actual=cred.test$response)
}

# calculating the accuracy
acc.knn = list()
for (i in 1:length(knn.model)) {
  acc.knn[i] <- sum(diag(pred.knn[[i]]))/250
}
```

```{r, comment=NA}
selected.knn <- knn.model[[which.max(acc.knn)]] # selected knn
paste("The selected knn model uses a distance of", selected.knn$distance) %>% paste("and is computed with the", ifelse(which.max(acc.knn)<20,which.max(acc.knn),which.max(acc.knn)-20)) %>% paste("neerest neighbors.") %>%
paste("The obtained accuracy is equal to", acc.knn[[which.max(acc.knn)]]) 

# confusion matrix
pred.knn[which.max(acc.knn)] %>% kable(caption = "Confusion matrix of the fitted KNN model",
                                          col.names = c("Predict good", "Predict bad")) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "l"
  ) 


```



Petit DB donc pas un pb que "lazy learner"




### Optimizing the model: selecting the number of nodes and layers

### Predicting the values of the testing set

Apres y'en a marre...






















# Cross Validation approach {#CV}

Creation of 10 different sets in order to do the Cross-Validation.
The number of instances being `r nrow(cred)`, we will make 10 sets of size `r nrow(cred)/10`. They are stored in a list named test.list. At each step, the remaining part of the data base is stored in the list train.test.

```{r}
test.list <- list() # creates an empty list that will be the test sets
train.list <- list() # creates an empty list that will be the train sets
counter <- 0

# Creates the 10 sets of size 300
for (i in 1:10){
  index <- counter + c(1:300) # the row numbers that will be in the test set
  test.list[[i]] <- cred[index, ] # the test set number i
  train.list[[i]] <- cred[-index, ] # the train set number i
  counter <- counter + 300
}
```

For example, *test.list[[1]]* is a data set of 300 rows taken at random from our data.

```{r} 
# test.list[[1]] %>% kable(caption="Our first test set named test.list[[1]]") %>% 
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
#   scroll_box(width = "100%", height = "200px")
# 
# 
# train.list[[1]] %>% kable(caption="Train set associated with test.list[[1]]") %>% 
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
#   scroll_box(width = "100%", height = "200px")
```


